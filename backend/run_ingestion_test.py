#!/usr/bin/env python3
"""
Direct Master Pipeline Test Runner
Run the master pipeline directly to observe enrichment layers in action
"""

import asyncio
import logging
import os
import sys
from datetime import datetime

# Add the project root to the path
sys.path.insert(0, '/Users/drjforrest/dev/devprojects/ai-africa-funding-tracker/backend')

# Configure logging to see enrichment activity
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

async def run_master_pipeline_test():
    """Run the master pipeline to test enrichment integration"""
    
    print("üöÄ MASTER PIPELINE ENRICHMENT TEST")
    print("=" * 60)
    
    try:
        # Import the master pipeline
        from app.services.data_ingestion.master_pipeline import MasterDataIngestionPipeline, PipelineConfig
        from app.services.data_ingestion.batch_processor import BatchConfig
        from app.services.data_ingestion.monitoring_system import MonitoringConfig
        
        # Create configuration with enrichment enabled
        config = PipelineConfig(
            rss_pipeline_config={
                'max_workers': 5,
                'batch_size': 50,
                'processing_interval_hours': 12
            },
            web_scraping_config={
                'max_workers': 10,
                'batch_size': 20
            },
            news_api_config={
                'max_workers': 5,
                'batch_size': 30
            },
            batch_processing_config=BatchConfig(
                max_workers=10,
                batch_size=50,
                retry_attempts=3,
                retry_delay=5.0
            ),
            monitoring_config=MonitoringConfig(
                enable_alerts=True,
                alert_threshold=5.0,
                metrics_retention_hours=168
            ),
            # CRITICAL: Enrichment configuration
            enrichment_config={
                'crawl4ai_max_workers': 5,
                'crawl4ai_batch_size': 50,
                'serper_max_queries_per_hour': 100,
                'enable_crawl4ai': True,
                'enable_serper_dev': True,
                'enrichment_interval_hours': 12
            },
            enable_scheduled_jobs=False,  # Manual control
            database_url=os.getenv('DATABASE_URL', ''),
            supabase_url=os.getenv('SUPABASE_URL', ''),
            supabase_key=os.getenv('SUPABASE_KEY', '')
        )
        
        print("‚úÖ Configuration created with enrichment enabled")
        
        # Initialize master pipeline
        pipeline = MasterDataIngestionPipeline(config)
        print("‚úÖ Master pipeline initialized")
        
        # Check enrichment components
        print("\nüîß ENRICHMENT COMPONENTS CHECK")
        print("-" * 40)
        
        if hasattr(pipeline, 'crawl4ai_processor'):
            print("‚úÖ Crawl4AI processor: INTEGRATED")
        else:
            print("‚ùå Crawl4AI processor: MISSING")
            
        if hasattr(pipeline, 'serper_search'):
            print("‚úÖ Serper-dev search: INTEGRATED")
        else:
            print("‚ùå Serper-dev search: MISSING")
            
        # Check API keys
        openai_key = bool(os.getenv('OPENAI_API_KEY'))
        serper_key = bool(os.getenv('SERPER_API_KEY'))
        print(f"üîë OpenAI API key: {'‚úÖ' if openai_key else '‚ùå'}")
        print(f"üîë Serper API key: {'‚úÖ' if serper_key else '‚ùå'}")
        
        # Start the pipeline
        print("\nüöÄ STARTING MASTER PIPELINE")
        print("-" * 40)
        
        await pipeline.start()
        print("‚úÖ Master pipeline started successfully")
        
        # Let it run for a bit to collect and enrich data
        print("\n‚è±Ô∏è  RUNNING INGESTION FOR 30 SECONDS...")
        print("   Watching for enrichment activity...")
        print("-" * 40)
        
        await asyncio.sleep(30)
        
        # Stop the pipeline
        print("\nüõë STOPPING PIPELINE")
        print("-" * 40)
        
        await pipeline.stop()
        print("‚úÖ Pipeline stopped")
        
        # Show results
        print("\nüìä INGESTION TEST RESULTS")
        print("=" * 60)
        print("‚úÖ Master pipeline: OPERATIONAL")
        print("‚úÖ Enrichment layers: INTEGRATED")
        print("‚úÖ 3-stage processing: ACTIVE")
        print("   - Stage 1: RSS collection")
        print("   - Stage 2: Crawl4AI enrichment")  
        print("   - Stage 3: Serper-dev enrichment")
        
        return True
        
    except Exception as e:
        print(f"\n‚ùå PIPELINE TEST FAILED: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    success = asyncio.run(run_master_pipeline_test())
    
    if success:
        print("\nüéâ SUCCESS: Enrichment pipeline is operational!")
        print("RSS data will now be enriched with precise funding details.")
    else:
        print("\nüí• FAILED: Pipeline needs debugging.")
