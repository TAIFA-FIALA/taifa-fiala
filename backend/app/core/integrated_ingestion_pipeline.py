"""
Integrated Ingestion Pipeline with Equity-Aware Processing
=========================================================

Complete integration of all components:
- Multi-source ingestion (RSS, Serper, User, Crawl4AI)
- Equity-aware content classification
- Vector database indexing
- Bias monitoring integration
- Full-text processing and metadata storage
- Priority source management

This module connects all the pieces together into a cohesive pipeline.
"""

import asyncio
import logging
from typing import Dict, List, Optional, Any, Tuple
from datetime import datetime
from dataclasses import dataclass, field
from enum import Enum
import json
import hashlib
import uuid
from concurrent.futures import ThreadPoolExecutor

from app.core.etl_architecture import ETLTask, PipelineStage, ProcessingResult
from app.core.equity_aware_classifier import EquityAwareContentClassifier, ClassificationResult
from app.core.vector_database import VectorDatabaseManager, VectorETLProcessor
from app.core.bias_monitoring import BiasMonitoringEngine
from app.core.multilingual_search import MultilingualSearchEngine
from app.core.priority_data_sources import PriorityDataSourceRegistry
from app.core.source_quality_scoring import SourceQualityScorer
from app.core.enhanced_duplicate_detection import EnhancedDuplicateDetector
from app.models.funding import AfricaIntelligenceItem
from app.models.validation import ValidationResult, ContentFingerprint
from app.core.database import get_db_session

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# =============================================================================
# INGESTION PIPELINE MODELS
# =============================================================================

class IngestionMethod(Enum):
    """Types of ingestion methods"""
    RSS_FEED = "rss_feed"
    SERPER_SEARCH = "serper_search"
    USER_SUBMISSION = "user_submission"
    CRAWL4AI_EXTRACTION = "crawl4ai_extraction"
    MULTILINGUAL_SEARCH = "multilingual_search"
    PRIORITY_SOURCE_SCAN = "priority_source_scan"

@dataclass
class IngestionContext:
    """Context information for ingestion"""
    method: IngestionMethod
    source_id: str
    source_name: str
    source_url: str
    source_language: str = "en"
    source_priority: float = 1.0
    processing_metadata: Dict[str, Any] = field(default_factory=dict)
    
@dataclass
class ProcessedContent:
    """Processed content with all metadata"""
    # Original content
    raw_content: Dict[str, Any]
    
    # Processing results
    classification: ClassificationResult
    validation: ValidationResult
    fingerprint: ContentFingerprint
    
    # Vector processing
    vector_indexed: bool = False
    vector_id: Optional[str] = None
    
    # Integration metadata
    ingestion_context: IngestionContext
    processing_timestamp: datetime = field(default_factory=datetime.now)
    processing_time: float = 0.0
    
    # Quality scores
    duplicate_score: float = 0.0
    source_quality_score: float = 0.0
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary for storage"""
        return {
            'raw_content': self.raw_content,
            'classification': self.classification.to_dict(),
            'validation': vars(self.validation),
            'fingerprint': vars(self.fingerprint),
            'vector_indexed': self.vector_indexed,
            'vector_id': self.vector_id,
            'ingestion_context': vars(self.ingestion_context),
            'processing_timestamp': self.processing_timestamp.isoformat(),
            'processing_time': self.processing_time,
            'duplicate_score': self.duplicate_score,
            'source_quality_score': self.source_quality_score
        }

# =============================================================================
# INTEGRATED INGESTION PIPELINE
# =============================================================================

class IntegratedIngestionPipeline:
    """Complete integrated ingestion pipeline with equity awareness"""
    
    def __init__(self, 
                 vector_manager: VectorDatabaseManager,
                 multilingual_engine: MultilingualSearchEngine,
                 serper_api_key: str):
        self.logger = logging.getLogger(__name__)
        
        # Initialize all components
        self.equity_classifier = EquityAwareContentClassifier()
        self.vector_manager = vector_manager
        self.vector_processor = VectorETLProcessor(vector_manager)
        self.bias_monitor = BiasMonitoringEngine()
        self.multilingual_engine = multilingual_engine
        self.source_registry = PriorityDataSourceRegistry()
        self.source_scorer = SourceQualityScorer()
        self.duplicate_detector = EnhancedDuplicateDetector()
        
        # Processing configuration
        self.batch_size = 50
        self.max_workers = 10
        self.executor = ThreadPoolExecutor(max_workers=self.max_workers)
        
        # Performance tracking
        self.processing_stats = {\n            'total_processed': 0,\n            'successful_classifications': 0,\n            'successful_indexing': 0,\n            'duplicates_detected': 0,\n            'bias_alerts_generated': 0\n        }\n    \n    async def process_content_batch(self, content_batch: List[Dict[str, Any]], \n                                  ingestion_context: IngestionContext) -> List[ProcessedContent]:\n        \"\"\"Process a batch of content through the complete pipeline\"\"\"\n        try:\n            start_time = datetime.now()\n            processed_items = []\n            \n            # Step 1: Classify content with equity awareness\n            self.logger.info(f\"Starting equity classification for {len(content_batch)} items\")\n            classification_tasks = [\n                self.equity_classifier.classify_content(content) \n                for content in content_batch\n            ]\n            classifications = await asyncio.gather(*classification_tasks, return_exceptions=True)\n            \n            # Step 2: Filter out failed classifications\n            valid_items = []\n            for i, (content, classification) in enumerate(zip(content_batch, classifications)):\n                if isinstance(classification, Exception):\n                    self.logger.error(f\"Classification failed for item {i}: {classification}\")\n                    continue\n                \n                valid_items.append((content, classification))\n                self.processing_stats['successful_classifications'] += 1\n            \n            # Step 3: Check for duplicates\n            self.logger.info(f\"Checking duplicates for {len(valid_items)} items\")\n            for content, classification in valid_items:\n                # Get existing content for duplicate detection\n                existing_content = await self._get_existing_content_for_duplicate_check()\n                \n                # Check for duplicates\n                duplicate_matches = await self.duplicate_detector.detect_duplicates(\n                    new_content=content,\n                    existing_content=existing_content\n                )\n                \n                # Skip if high-confidence duplicate\n                if self._is_high_confidence_duplicate(duplicate_matches):\n                    self.logger.info(f\"Skipping duplicate: {content.get('title', 'No title')}\")\n                    self.processing_stats['duplicates_detected'] += 1\n                    continue\n                \n                # Step 4: Create validation result\n                validation_result = await self._create_validation_result(\n                    content, classification, duplicate_matches\n                )\n                \n                # Step 5: Create content fingerprint\n                fingerprint = await self._create_content_fingerprint(\n                    content, classification\n                )\n                \n                # Step 6: Create processed content object\n                processed_content = ProcessedContent(\n                    raw_content=content,\n                    classification=classification,\n                    validation=validation_result,\n                    fingerprint=fingerprint,\n                    ingestion_context=ingestion_context,\n                    duplicate_score=self._calculate_duplicate_score(duplicate_matches),\n                    source_quality_score=await self._get_source_quality_score(ingestion_context.source_id)\n                )\n                \n                processed_items.append(processed_content)\n            \n            # Step 7: Vector indexing for approved items\n            self.logger.info(f\"Vector indexing for {len(processed_items)} items\")\n            await self._batch_vector_index(processed_items)\n            \n            # Step 8: Store in database\n            self.logger.info(f\"Storing {len(processed_items)} items in database\")\n            await self._batch_store_processed_content(processed_items)\n            \n            # Step 9: Update bias monitoring\n            await self._update_bias_monitoring(processed_items)\n            \n            # Step 10: Update source quality scores\n            await self._update_source_quality_scores(ingestion_context, processed_items)\n            \n            # Calculate processing time\n            processing_time = (datetime.now() - start_time).total_seconds()\n            \n            # Update processing stats\n            self.processing_stats['total_processed'] += len(processed_items)\n            \n            self.logger.info(f\"Batch processing completed: {len(processed_items)} items in {processing_time:.2f}s\")\n            \n            return processed_items\n            \n        except Exception as e:\n            self.logger.error(f\"Batch processing failed: {e}\")\n            return []\n    \n    async def process_rss_feed(self, feed_url: str, source_config: Dict[str, Any]) -> List[ProcessedContent]:\n        \"\"\"Process RSS feed with full integration\"\"\"\n        try:\n            # Create ingestion context\n            context = IngestionContext(\n                method=IngestionMethod.RSS_FEED,\n                source_id=source_config.get('source_id', 'unknown'),\n                source_name=source_config.get('source_name', 'RSS Feed'),\n                source_url=feed_url,\n                source_language=source_config.get('language', 'en'),\n                source_priority=source_config.get('priority', 1.0),\n                processing_metadata={'feed_url': feed_url}\n            )\n            \n            # Fetch RSS content\n            rss_content = await self._fetch_rss_content(feed_url)\n            \n            # Process through pipeline\n            return await self.process_content_batch(rss_content, context)\n            \n        except Exception as e:\n            self.logger.error(f\"RSS processing failed for {feed_url}: {e}\")\n            return []\n    \n    async def process_serper_search(self, query: str, search_config: Dict[str, Any]) -> List[ProcessedContent]:\n        \"\"\"Process Serper search results with full integration\"\"\"\n        try:\n            # Create ingestion context\n            context = IngestionContext(\n                method=IngestionMethod.SERPER_SEARCH,\n                source_id=f\"serper_{hashlib.md5(query.encode()).hexdigest()[:8]}\",\n                source_name=f\"Serper Search: {query}\",\n                source_url=\"https://google.serper.dev/search\",\n                source_language=search_config.get('language', 'en'),\n                source_priority=search_config.get('priority', 1.0),\n                processing_metadata={'query': query, 'search_config': search_config}\n            )\n            \n            # Execute search\n            search_results = await self._execute_serper_search(query, search_config)\n            \n            # Process through pipeline\n            return await self.process_content_batch(search_results, context)\n            \n        except Exception as e:\n            self.logger.error(f\"Serper search processing failed for '{query}': {e}\")\n            return []\n    \n    async def process_multilingual_search(self, base_query: str, \n                                        target_languages: List[str]) -> List[ProcessedContent]:\n        \"\"\"Process multilingual search with full integration\"\"\"\n        try:\n            all_processed = []\n            \n            # Execute multilingual search\n            from app.core.multilingual_search import SupportedLanguage\n            languages = [SupportedLanguage(lang) for lang in target_languages]\n            \n            multilingual_results = await self.multilingual_engine.search_multilingual(\n                base_query=base_query,\n                target_languages=languages\n            )\n            \n            # Process each language result\n            for result in multilingual_results:\n                context = IngestionContext(\n                    method=IngestionMethod.MULTILINGUAL_SEARCH,\n                    source_id=f\"multilingual_{result.query.language.value}\",\n                    source_name=f\"Multilingual Search ({result.query.language.value})\",\n                    source_url=\"multilingual_search\",\n                    source_language=result.query.language.value,\n                    source_priority=1.5,  # Higher priority for multilingual\n                    processing_metadata={\n                        'base_query': base_query,\n                        'translated_query': result.query.translated_query,\n                        'confidence_score': result.confidence_score\n                    }\n                )\n                \n                processed = await self.process_content_batch(result.results, context)\n                all_processed.extend(processed)\n            \n            return all_processed\n            \n        except Exception as e:\n            self.logger.error(f\"Multilingual search processing failed: {e}\")\n            return []\n    \n    async def process_priority_sources(self, max_sources: int = 10) -> List[ProcessedContent]:\n        \"\"\"Process content from priority African data sources\"\"\"\n        try:\n            all_processed = []\n            \n            # Get high-priority sources\n            priority_sources = self.source_registry.get_high_priority_sources()\n            \n            # Process each source\n            for source in priority_sources[:max_sources]:\n                self.logger.info(f\"Processing priority source: {source.name}\")\n                \n                # Create ingestion context\n                context = IngestionContext(\n                    method=IngestionMethod.PRIORITY_SOURCE_SCAN,\n                    source_id=source.source_id,\n                    source_name=source.name,\n                    source_url=source.base_url,\n                    source_language=source.primary_language.value,\n                    source_priority=source.priority_weight,\n                    processing_metadata={\n                        'source_config': source.to_dict()\n                    }\n                )\n                \n                # Process RSS feeds\n                for feed_url in source.rss_feeds:\n                    try:\n                        rss_content = await self._fetch_rss_content(feed_url)\n                        processed = await self.process_content_batch(rss_content, context)\n                        all_processed.extend(processed)\n                    except Exception as e:\n                        self.logger.error(f\"RSS processing failed for {feed_url}: {e}\")\n                \n                # Process search pages with multilingual queries\n                if source.search_pages:\n                    try:\n                        search_results = await self._process_search_pages(\n                            source.search_pages, \n                            source.primary_language.value\n                        )\n                        processed = await self.process_content_batch(search_results, context)\n                        all_processed.extend(processed)\n                    except Exception as e:\n                        self.logger.error(f\"Search page processing failed: {e}\")\n            \n            return all_processed\n            \n        except Exception as e:\n            self.logger.error(f\"Priority source processing failed: {e}\")\n            return []\n    \n    async def process_user_submission(self, submission_data: Dict[str, Any]) -> ProcessedContent:\n        \"\"\"Process user-submitted content with full integration\"\"\"\n        try:\n            # Create ingestion context\n            context = IngestionContext(\n                method=IngestionMethod.USER_SUBMISSION,\n                source_id=f\"user_{submission_data.get('user_id', 'anonymous')}\",\n                source_name=\"User Submission\",\n                source_url=submission_data.get('url', 'user_submission'),\n                source_language=submission_data.get('language', 'en'),\n                source_priority=0.8,  # Lower priority for user submissions\n                processing_metadata={\n                    'user_id': submission_data.get('user_id'),\n                    'submission_type': submission_data.get('type', 'manual')\n                }\n            )\n            \n            # Process single item\n            processed_batch = await self.process_content_batch([submission_data], context)\n            \n            return processed_batch[0] if processed_batch else None\n            \n        except Exception as e:\n            self.logger.error(f\"User submission processing failed: {e}\")\n            return None\n    \n    # =============================================================================\n    # PRIVATE HELPER METHODS\n    # =============================================================================\n    \n    async def _fetch_rss_content(self, feed_url: str) -> List[Dict[str, Any]]:\n        \"\"\"Fetch and parse RSS content\"\"\"\n        try:\n            import feedparser\n            import aiohttp\n            \n            async with aiohttp.ClientSession() as session:\n                async with session.get(feed_url) as response:\n                    if response.status == 200:\n                        feed_data = await response.text()\n                        feed = feedparser.parse(feed_data)\n                        \n                        content_items = []\n                        for entry in feed.entries:\n                            content_items.append({\n                                'title': entry.get('title', ''),\n                                'description': entry.get('description', ''),\n                                'url': entry.get('link', ''),\n                                'published_date': entry.get('published', ''),\n                                'author': entry.get('author', ''),\n                                'source': feed_url,\n                                'raw_entry': entry\n                            })\n                        \n                        return content_items\n                    else:\n                        self.logger.error(f\"RSS fetch failed: {response.status}\")\n                        return []\n                        \n        except Exception as e:\n            self.logger.error(f\"RSS content fetch failed: {e}\")\n            return []\n    \n    async def _execute_serper_search(self, query: str, config: Dict[str, Any]) -> List[Dict[str, Any]]:\n        \"\"\"Execute Serper search with configuration\"\"\"\n        try:\n            import aiohttp\n            \n            # Prepare search parameters\n            params = {\n                'q': query,\n                'num': config.get('num_results', 20),\n                'hl': config.get('language', 'en'),\n                'gl': config.get('country', 'za'),  # Default to South Africa\n                'type': 'search'\n            }\n            \n            headers = {\n                'X-API-KEY': config.get('api_key', ''),\n                'Content-Type': 'application/json'\n            }\n            \n            async with aiohttp.ClientSession() as session:\n                async with session.post(\n                    'https://google.serper.dev/search',\n                    json=params,\n                    headers=headers\n                ) as response:\n                    if response.status == 200:\n                        data = await response.json()\n                        \n                        search_results = []\n                        for result in data.get('organic', []):\n                            search_results.append({\n                                'title': result.get('title', ''),\n                                'description': result.get('snippet', ''),\n                                'url': result.get('link', ''),\n                                'source': result.get('source', ''),\n                                'date': result.get('date', ''),\n                                'position': result.get('position', 0),\n                                'raw_result': result\n                            })\n                        \n                        return search_results\n                    else:\n                        self.logger.error(f\"Serper search failed: {response.status}\")\n                        return []\n                        \n        except Exception as e:\n            self.logger.error(f\"Serper search execution failed: {e}\")\n            return []\n    \n    async def _process_search_pages(self, search_pages: List[str], language: str) -> List[Dict[str, Any]]:\n        \"\"\"Process search pages with web scraping\"\"\"\n        try:\n            # This would use Crawl4AI or similar for web scraping\n            # For now, return empty list\n            return []\n            \n        except Exception as e:\n            self.logger.error(f\"Search page processing failed: {e}\")\n            return []\n    \n    async def _get_existing_content_for_duplicate_check(self) -> List[Dict[str, Any]]:\n        \"\"\"Get existing content for duplicate detection\"\"\"\n        try:\n            async with get_db_session() as session:\n                # Get recent opportunities for duplicate checking\n                query = \"\"\"\n                    SELECT id, title, description, url, organization_name, funding_amount\n                    FROM africa_intelligence_feed\n                    WHERE discovered_date >= (NOW() - INTERVAL '30 days')\n                    ORDER BY discovered_date DESC\n                    LIMIT 1000\n                \"\"\"\n                \n                result = await session.execute(query)\n                return [dict(row) for row in result.fetchall()]\n                \n        except Exception as e:\n            self.logger.error(f\"Getting existing content failed: {e}\")\n            return []\n    \n    def _is_high_confidence_duplicate(self, duplicate_matches: List[Any]) -> bool:\n        \"\"\"Check if content is a high-confidence duplicate\"\"\"\n        if not duplicate_matches:\n            return False\n        \n        # Check for high-confidence exact matches\n        for match in duplicate_matches:\n            if match.confidence_score > 0.9 and match.action.value == 'reject':\n                return True\n        \n        return False\n    \n    async def _create_validation_result(self, content: Dict[str, Any], \n                                      classification: ClassificationResult,\n                                      duplicate_matches: List[Any]) -> ValidationResult:\n        \"\"\"Create validation result from processing\"\"\"\n        try:\n            # Calculate confidence based on classification and duplicates\n            base_confidence = classification.confidence_score\n            \n            # Adjust for duplicates\n            if duplicate_matches:\n                duplicate_penalty = min(0.3, len(duplicate_matches) * 0.1)\n                base_confidence = max(0.0, base_confidence - duplicate_penalty)\n            \n            # Determine status\n            if base_confidence >= 0.85:\n                status = 'auto_approved'\n                requires_review = False\n            elif base_confidence >= 0.65:\n                status = 'pending'\n                requires_review = True\n            else:\n                status = 'rejected'\n                requires_review = False\n            \n            return ValidationResult(\n                id=str(uuid.uuid4()),\n                status=status,\n                confidence_score=base_confidence,\n                confidence_level=self._get_confidence_level(base_confidence),\n                completeness_score=self._calculate_completeness_score(content),\n                relevance_score=classification.equity_score.sectoral_score,\n                legitimacy_score=classification.equity_score.transparency_score,\n                validation_notes=f\"Equity score: {classification.equity_score.calculate_overall_score():.3f}\",\n                requires_human_review=requires_review,\n                validator='integrated_pipeline',\n                processing_time=0.0,  # Will be updated later\n                validated_data=classification.to_dict(),\n                created_at=datetime.now()\n            )\n            \n        except Exception as e:\n            self.logger.error(f\"Creating validation result failed: {e}\")\n            return ValidationResult(\n                id=str(uuid.uuid4()),\n                status='error',\n                confidence_score=0.0,\n                created_at=datetime.now()\n            )\n    \n    async def _create_content_fingerprint(self, content: Dict[str, Any], \n                                        classification: ClassificationResult) -> ContentFingerprint:\n        \"\"\"Create content fingerprint for duplicate detection\"\"\"\n        try:\n            title = content.get('title', '')\n            description = content.get('description', '')\n            url = content.get('url', '')\n            \n            # Generate hashes\n            title_hash = hashlib.md5(title.lower().encode()).hexdigest()\n            content_hash = hashlib.md5(f\"{title} {description}\".lower().encode()).hexdigest()\n            url_hash = hashlib.md5(url.encode()).hexdigest() if url else ''\n            \n            # Create signature hash\n            signature_parts = [\n                title_hash,\n                classification.detected_countries[0] if classification.detected_countries else '',\n                classification.detected_sectors[0] if classification.detected_sectors else '',\n                str(content.get('funding_amount', ''))\n            ]\n            signature_hash = hashlib.md5('|'.join(signature_parts).encode()).hexdigest()\n            \n            return ContentFingerprint(\n                title_hash=title_hash,\n                content_hash=content_hash,\n                url_hash=url_hash,\n                signature_hash=signature_hash,\n                organization_name=content.get('organization_name', ''),\n                funding_amount=content.get('funding_amount'),\n                funding_currency=content.get('currency', 'USD'),\n                url_domain=content.get('url', '').split('/')[2] if content.get('url') else None,\n                key_phrases=classification.detected_sectors + classification.detected_countries,\n                created_at=datetime.now()\n            )\n            \n        except Exception as e:\n            self.logger.error(f\"Creating content fingerprint failed: {e}\")\n            return ContentFingerprint(\n                title_hash='',\n                content_hash='',\n                url_hash='',\n                signature_hash='',\n                created_at=datetime.now()\n            )\n    \n    def _calculate_duplicate_score(self, duplicate_matches: List[Any]) -> float:\n        \"\"\"Calculate duplicate score from matches\"\"\"\n        if not duplicate_matches:\n            return 0.0\n        \n        # Get highest confidence match\n        max_confidence = max(match.confidence_score for match in duplicate_matches)\n        return max_confidence\n    \n    async def _get_source_quality_score(self, source_id: str) -> float:\n        \"\"\"Get source quality score\"\"\"\n        try:\n            # This would integrate with the source quality scorer\n            # For now, return default score\n            return 0.8\n            \n        except Exception as e:\n            self.logger.error(f\"Getting source quality score failed: {e}\")\n            return 0.5\n    \n    async def _batch_vector_index(self, processed_items: List[ProcessedContent]):\n        \"\"\"Batch index processed items in vector database\"\"\"\n        try:\n            # Filter approved items for vector indexing\n            approved_items = [\n                item for item in processed_items\n                if item.validation.status in ['approved', 'auto_approved']\n            ]\n            \n            if not approved_items:\n                return\n            \n            # Create intelligence feed for vector indexing\n            opportunities = []\n            for item in approved_items:\n                opportunity = self._create_intelligence_item_from_processed(item)\n                if opportunity:\n                    opportunities.append(opportunity)\n            \n            # Batch index in vector database\n            if opportunities:\n                result = await self.vector_processor.batch_process_opportunities(opportunities)\n                \n                # Update vector indexing status\n                for i, item in enumerate(approved_items):\n                    if i < len(opportunities):\n                        item.vector_indexed = result.success\n                        item.vector_id = f\"opportunity_{opportunities[i].id}\"\n                        self.processing_stats['successful_indexing'] += 1\n                        \n        except Exception as e:\n            self.logger.error(f\"Batch vector indexing failed: {e}\")\n    \n    def _create_intelligence_item_from_processed(self, item: ProcessedContent) -> Optional[AfricaIntelligenceItem]:\n        \"\"\"Create AfricaIntelligenceItem from processed content\"\"\"\n        try:\n            # This would create a proper AfricaIntelligenceItem object\n            # For now, return None to indicate this needs implementation\n            return None\n            \n        except Exception as e:\n            self.logger.error(f\"Creating intelligence item failed: {e}\")\n            return None\n    \n    async def _batch_store_processed_content(self, processed_items: List[ProcessedContent]):\n        \"\"\"Store processed content in database\"\"\"\n        try:\n            # This would store all the processed content with metadata\n            # For now, just log the storage\n            self.logger.info(f\"Storing {len(processed_items)} processed items\")\n            \n        except Exception as e:\n            self.logger.error(f\"Batch storage failed: {e}\")\n    \n    async def _update_bias_monitoring(self, processed_items: List[ProcessedContent]):\n        \"\"\"Update bias monitoring with processed items\"\"\"\n        try:\n            # Extract bias-relevant data\n            bias_data = []\n            for item in processed_items:\n                bias_data.append({\n                    'countries': item.classification.detected_countries,\n                    'sectors': item.classification.detected_sectors,\n                    'inclusion_indicators': [cat.value for cat in item.classification.inclusion_indicators],\n                    'equity_score': item.classification.equity_score.calculate_overall_score(),\n                    'source_language': item.ingestion_context.source_language,\n                    'funding_stage': item.classification.funding_stage\n                })\n            \n            # Analyze current bias (this would trigger monitoring)\n            snapshot = await self.bias_monitor.analyze_current_bias()\n            \n            # Check for alerts\n            if snapshot.active_alerts:\n                self.processing_stats['bias_alerts_generated'] += len(snapshot.active_alerts)\n                \n                # Log alerts\n                for alert in snapshot.active_alerts:\n                    self.logger.warning(f\"Bias alert: {alert.message}\")\n                    \n                    # Trigger mitigation if critical\n                    if alert.severity.value == 'critical':\n                        await self.bias_monitor.trigger_bias_mitigation(\n                            alert.bias_type, alert.severity\n                        )\n                        \n        except Exception as e:\n            self.logger.error(f\"Bias monitoring update failed: {e}\")\n    \n    async def _update_source_quality_scores(self, context: IngestionContext, \n                                          processed_items: List[ProcessedContent]):\n        \"\"\"Update source quality scores based on processing results\"\"\"\n        try:\n            # Calculate source performance metrics\n            total_items = len(processed_items)\n            if total_items == 0:\n                return\n            \n            successful_items = sum(1 for item in processed_items \n                                 if item.validation.status in ['approved', 'auto_approved'])\n            \n            duplicate_items = sum(1 for item in processed_items \n                                if item.duplicate_score > 0.8)\n            \n            # Update source quality score\n            from app.core.source_quality_scoring import SourceType\n            \n            # Map ingestion method to source type\n            source_type_map = {\n                IngestionMethod.RSS_FEED: SourceType.RSS_FEED,\n                IngestionMethod.SERPER_SEARCH: SourceType.API,\n                IngestionMethod.USER_SUBMISSION: SourceType.USER_SUBMISSION,\n                IngestionMethod.MULTILINGUAL_SEARCH: SourceType.API,\n                IngestionMethod.PRIORITY_SOURCE_SCAN: SourceType.WEBSITE\n            }\n            \n            source_type = source_type_map.get(context.method, SourceType.WEBSITE)\n            \n            # Score the source\n            snapshot = await self.source_scorer.score_source(\n                source_name=context.source_name,\n                source_type=source_type\n            )\n            \n            self.logger.info(f\"Source quality updated: {context.source_name} - {snapshot.metrics.calculate_overall_score():.3f}\")\n            \n        except Exception as e:\n            self.logger.error(f\"Source quality update failed: {e}\")\n    \n    def _get_confidence_level(self, score: float) -> str:\n        \"\"\"Get confidence level from score\"\"\"\n        if score >= 0.9:\n            return 'very_high'\n        elif score >= 0.8:\n            return 'high'\n        elif score >= 0.6:\n            return 'medium'\n        elif score >= 0.4:\n            return 'low'\n        else:\n            return 'very_low'\n    \n    def _calculate_completeness_score(self, content: Dict[str, Any]) -> float:\n        \"\"\"Calculate completeness score for content\"\"\"\n        required_fields = ['title', 'description', 'url']\n        optional_fields = ['organization_name', 'funding_amount', 'deadline', 'application_url']\n        \n        required_score = sum(1 for field in required_fields if content.get(field))\n        optional_score = sum(1 for field in optional_fields if content.get(field))\n        \n        # Weight required fields more heavily\n        total_score = (required_score / len(required_fields)) * 0.7 + (optional_score / len(optional_fields)) * 0.3\n        \n        return total_score\n    \n    def get_processing_stats(self) -> Dict[str, Any]:\n        \"\"\"Get current processing statistics\"\"\"\n        return {\n            **self.processing_stats,\n            'timestamp': datetime.now().isoformat(),\n            'pipeline_status': 'active'\n        }\n    \n    def reset_processing_stats(self):\n        \"\"\"Reset processing statistics\"\"\"\n        self.processing_stats = {\n            'total_processed': 0,\n            'successful_classifications': 0,\n            'successful_indexing': 0,\n            'duplicates_detected': 0,\n            'bias_alerts_generated': 0\n        }\n\n# =============================================================================\n# USAGE EXAMPLE\n# =============================================================================\n\nasync def example_usage():\n    \"\"\"Example usage of integrated ingestion pipeline\"\"\"\n    from app.core.vector_database import VectorConfig, VectorDatabaseManager\n    from app.core.multilingual_search import MultilingualSearchEngine\n    \n    # Initialize components\n    vector_config = VectorConfig(pinecone_api_key=\"your-key\")\n    vector_manager = VectorDatabaseManager(vector_config)\n    await vector_manager.initialize()\n    \n    multilingual_engine = MultilingualSearchEngine(serper_api_key=\"your-key\")\n    \n    # Create integrated pipeline\n    pipeline = IntegratedIngestionPipeline(\n        vector_manager=vector_manager,\n        multilingual_engine=multilingual_engine,\n        serper_api_key=\"your-key\"\n    )\n    \n    # Process RSS feed\n    rss_results = await pipeline.process_rss_feed(\n        feed_url=\"https://www.afdb.org/en/news-and-events/feed\",\n        source_config={\n            'source_id': 'afdb',\n            'source_name': 'African Development Bank',\n            'language': 'en',\n            'priority': 2.0\n        }\n    )\n    \n    print(f\"RSS processing: {len(rss_results)} items\")\n    \n    # Process multilingual search\n    multilingual_results = await pipeline.process_multilingual_search(\n        base_query=\"AI healthcare funding Africa\",\n        target_languages=['en', 'fr', 'ar']\n    )\n    \n    print(f\"Multilingual search: {len(multilingual_results)} items\")\n    \n    # Process priority sources\n    priority_results = await pipeline.process_priority_sources(max_sources=5)\n    \n    print(f\"Priority sources: {len(priority_results)} items\")\n    \n    # Get processing statistics\n    stats = pipeline.get_processing_stats()\n    print(f\"Processing stats: {stats}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(example_usage())"