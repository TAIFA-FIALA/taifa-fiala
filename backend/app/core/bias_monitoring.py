"""
Real-time Bias Monitoring System
================================

Comprehensive monitoring system to track and alert on bias patterns
in the AI Africa Funding Tracker ingestion pipeline.

Monitors:
- Geographic distribution bias
- Sectoral representation gaps
- Gender and inclusion metrics
- Language diversity
- Funding stage distribution
- Source quality equity
"""

import asyncio
import logging
from typing import Dict, List, Optional, Any, Tuple
from datetime import datetime, timedelta
from dataclasses import dataclass, field
from enum import Enum
import json
from statistics import mean, stdev
import pandas as pd
from collections import defaultdict, Counter

from app.core.database import get_db_session
from app.core.equity_aware_classifier import SupportedLanguage, GeographicTier, SectorPriority, InclusionCategory

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# =============================================================================
# BIAS MONITORING MODELS
# =============================================================================

class BiasType(Enum):
    """Types of bias being monitored"""
    GEOGRAPHIC = "geographic"
    SECTORAL = "sectoral"
    GENDER = "gender"
    LANGUAGE = "language"
    FUNDING_STAGE = "funding_stage"
    SOURCE_QUALITY = "source_quality"
    TEMPORAL = "temporal"

class AlertSeverity(Enum):
    """Alert severity levels"""
    CRITICAL = "critical"
    HIGH = "high"
    MEDIUM = "medium"
    LOW = "low"
    INFO = "info"

class BiasDirection(Enum):
    """Direction of bias"""
    OVER_REPRESENTED = "over_represented"
    UNDER_REPRESENTED = "under_represented"
    BALANCED = "balanced"

@dataclass
class BiasMetric:
    """Individual bias metric"""
    metric_name: str
    current_value: float
    target_value: float
    threshold_warning: float
    threshold_critical: float
    direction: BiasDirection
    trend_7d: float = 0.0
    trend_30d: float = 0.0
    
    def calculate_severity(self) -> AlertSeverity:
        """Calculate alert severity based on deviation from target"""
        deviation = abs(self.current_value - self.target_value)
        
        if deviation >= self.threshold_critical:
            return AlertSeverity.CRITICAL
        elif deviation >= self.threshold_warning:
            return AlertSeverity.HIGH
        elif deviation >= self.threshold_warning * 0.5:
            return AlertSeverity.MEDIUM
        else:
            return AlertSeverity.LOW

@dataclass
class BiasAlert:
    """Bias alert notification"""
    alert_id: str
    bias_type: BiasType
    severity: AlertSeverity
    message: str
    metric: BiasMetric
    recommendations: List[str] = field(default_factory=list)
    affected_regions: List[str] = field(default_factory=list)
    affected_sectors: List[str] = field(default_factory=list)
    timestamp: datetime = field(default_factory=datetime.now)
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary"""
        return {
            'alert_id': self.alert_id,
            'bias_type': self.bias_type.value,
            'severity': self.severity.value,
            'message': self.message,
            'metric': {
                'name': self.metric.metric_name,
                'current_value': self.metric.current_value,
                'target_value': self.metric.target_value,
                'direction': self.metric.direction.value,
                'trend_7d': self.metric.trend_7d,
                'trend_30d': self.metric.trend_30d
            },
            'recommendations': self.recommendations,
            'affected_regions': self.affected_regions,
            'affected_sectors': self.affected_sectors,
            'timestamp': self.timestamp.isoformat()
        }

@dataclass
class BiasSnapshot:
    """Comprehensive bias snapshot"""
    timestamp: datetime
    geographic_metrics: Dict[str, BiasMetric]
    sectoral_metrics: Dict[str, BiasMetric]
    inclusion_metrics: Dict[str, BiasMetric]
    language_metrics: Dict[str, BiasMetric]
    stage_metrics: Dict[str, BiasMetric]
    source_metrics: Dict[str, BiasMetric]
    overall_equity_score: float = 0.0
    active_alerts: List[BiasAlert] = field(default_factory=list)
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary"""
        return {
            'timestamp': self.timestamp.isoformat(),
            'overall_equity_score': self.overall_equity_score,
            'geographic_metrics': {k: vars(v) for k, v in self.geographic_metrics.items()},
            'sectoral_metrics': {k: vars(v) for k, v in self.sectoral_metrics.items()},
            'inclusion_metrics': {k: vars(v) for k, v in self.inclusion_metrics.items()},
            'language_metrics': {k: vars(v) for k, v in self.language_metrics.items()},
            'stage_metrics': {k: vars(v) for k, v in self.stage_metrics.items()},
            'source_metrics': {k: vars(v) for k, v in self.source_metrics.items()},
            'active_alerts': [alert.to_dict() for alert in self.active_alerts]
        }

# =============================================================================
# BIAS MONITORING ENGINE
# =============================================================================

class BiasMonitoringEngine:
    """Real-time bias monitoring and alerting system"""
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)
        
        # Bias targets and thresholds
        self.bias_targets = {
            'geographic': {
                'non_big_four_percentage': {
                    'target': 0.35,  # 35% from non-Big 4 countries
                    'warning': 0.15,  # Alert if deviation > 15%
                    'critical': 0.25  # Critical if deviation > 25%
                },
                'central_africa_percentage': {
                    'target': 0.15,  # 15% from Central Africa
                    'warning': 0.10,
                    'critical': 0.15
                },
                'west_africa_percentage': {
                    'target': 0.25,  # 25% from West Africa
                    'warning': 0.10,
                    'critical': 0.15
                },
                'geographic_diversity_index': {
                    'target': 0.7,   # Diversity index target
                    'warning': 0.15,
                    'critical': 0.25
                }
            },
            'sectoral': {
                'healthcare_percentage': {
                    'target': 0.15,  # 15% healthcare opportunities
                    'warning': 0.05,
                    'critical': 0.08
                },
                'agriculture_percentage': {
                    'target': 0.15,  # 15% agriculture opportunities
                    'warning': 0.05,
                    'critical': 0.08
                },
                'climate_percentage': {
                    'target': 0.10,  # 10% climate opportunities
                    'warning': 0.05,
                    'critical': 0.08
                },
                'sector_diversity_index': {
                    'target': 0.8,   # Sector diversity target
                    'warning': 0.15,
                    'critical': 0.25
                }
            },
            'inclusion': {
                'women_focused_percentage': {
                    'target': 0.20,  # 20% women-focused opportunities
                    'warning': 0.08,
                    'critical': 0.12
                },
                'youth_focused_percentage': {
                    'target': 0.25,  # 25% youth-focused opportunities
                    'warning': 0.10,
                    'critical': 0.15
                },
                'rural_focused_percentage': {
                    'target': 0.15,  # 15% rural-focused opportunities
                    'warning': 0.08,
                    'critical': 0.12
                },
                'inclusion_diversity_index': {
                    'target': 0.6,   # Inclusion diversity target
                    'warning': 0.15,
                    'critical': 0.25
                }
            },
            'language': {
                'non_english_percentage': {
                    'target': 0.20,  # 20% non-English opportunities
                    'warning': 0.10,
                    'critical': 0.15
                },
                'french_percentage': {
                    'target': 0.10,  # 10% French opportunities
                    'warning': 0.05,
                    'critical': 0.08
                },
                'arabic_percentage': {
                    'target': 0.05,  # 5% Arabic opportunities
                    'warning': 0.03,
                    'critical': 0.05
                },
                'language_diversity_index': {
                    'target': 0.5,   # Language diversity target
                    'warning': 0.15,
                    'critical': 0.25
                }
            },
            'funding_stage': {
                'early_stage_percentage': {
                    'target': 0.40,  # 40% early-stage opportunities
                    'warning': 0.15,
                    'critical': 0.20
                },
                'seed_stage_percentage': {
                    'target': 0.25,  # 25% seed-stage opportunities
                    'warning': 0.10,
                    'critical': 0.15
                },
                'grant_percentage': {
                    'target': 0.30,  # 30% grant opportunities
                    'warning': 0.10,
                    'critical': 0.15
                },
                'stage_diversity_index': {
                    'target': 0.7,   # Stage diversity target
                    'warning': 0.15,
                    'critical': 0.25
                }
            }
        }\n        \n        # Historical data for trend analysis\n        self.historical_snapshots = []\n        self.max_history_days = 90\n        \n        # Alert configuration\n        self.alert_cooldown_hours = 6  # Minimum time between similar alerts\n        self.recent_alerts = []\n    \n    async def analyze_current_bias(self) -> BiasSnapshot:\n        \"\"\"Analyze current bias in the system\"\"\"\n        try:\n            # Get current data\n            current_data = await self._get_current_ingestion_data()\n            \n            # Calculate metrics for each bias type\n            geographic_metrics = await self._calculate_geographic_metrics(current_data)\n            sectoral_metrics = await self._calculate_sectoral_metrics(current_data)\n            inclusion_metrics = await self._calculate_inclusion_metrics(current_data)\n            language_metrics = await self._calculate_language_metrics(current_data)\n            stage_metrics = await self._calculate_stage_metrics(current_data)\n            source_metrics = await self._calculate_source_metrics(current_data)\n            \n            # Calculate overall equity score\n            overall_equity = await self._calculate_overall_equity_score({\n                'geographic': geographic_metrics,\n                'sectoral': sectoral_metrics,\n                'inclusion': inclusion_metrics,\n                'language': language_metrics,\n                'stage': stage_metrics,\n                'source': source_metrics\n            })\n            \n            # Generate alerts\n            alerts = await self._generate_alerts({\n                'geographic': geographic_metrics,\n                'sectoral': sectoral_metrics,\n                'inclusion': inclusion_metrics,\n                'language': language_metrics,\n                'stage': stage_metrics,\n                'source': source_metrics\n            })\n            \n            # Create snapshot\n            snapshot = BiasSnapshot(\n                timestamp=datetime.now(),\n                geographic_metrics=geographic_metrics,\n                sectoral_metrics=sectoral_metrics,\n                inclusion_metrics=inclusion_metrics,\n                language_metrics=language_metrics,\n                stage_metrics=stage_metrics,\n                source_metrics=source_metrics,\n                overall_equity_score=overall_equity,\n                active_alerts=alerts\n            )\n            \n            # Store snapshot\n            await self._store_snapshot(snapshot)\n            \n            return snapshot\n            \n        except Exception as e:\n            self.logger.error(f\"Bias analysis failed: {e}\")\n            return BiasSnapshot(timestamp=datetime.now(), geographic_metrics={}, sectoral_metrics={}, \n                              inclusion_metrics={}, language_metrics={}, stage_metrics={}, source_metrics={})\n    \n    async def get_bias_trends(self, days: int = 30) -> Dict[str, Any]:\n        \"\"\"Get bias trends over specified period\"\"\"\n        try:\n            # Get historical snapshots\n            historical_data = await self._get_historical_snapshots(days)\n            \n            if not historical_data:\n                return {'error': 'No historical data available'}\n            \n            trends = {\n                'geographic_trends': self._calculate_metric_trends(historical_data, 'geographic_metrics'),\n                'sectoral_trends': self._calculate_metric_trends(historical_data, 'sectoral_metrics'),\n                'inclusion_trends': self._calculate_metric_trends(historical_data, 'inclusion_metrics'),\n                'language_trends': self._calculate_metric_trends(historical_data, 'language_metrics'),\n                'stage_trends': self._calculate_metric_trends(historical_data, 'stage_metrics'),\n                'overall_equity_trend': self._calculate_equity_trend(historical_data)\n            }\n            \n            return trends\n            \n        except Exception as e:\n            self.logger.error(f\"Getting bias trends failed: {e}\")\n            return {'error': str(e)}\n    \n    async def get_bias_recommendations(self) -> List[Dict[str, Any]]:\n        \"\"\"Get recommendations for addressing bias issues\"\"\"\n        try:\n            # Get current snapshot\n            current_snapshot = await self.analyze_current_bias()\n            \n            recommendations = []\n            \n            # Analyze geographic bias\n            geo_recs = await self._analyze_geographic_bias(current_snapshot.geographic_metrics)\n            recommendations.extend(geo_recs)\n            \n            # Analyze sectoral bias\n            sector_recs = await self._analyze_sectoral_bias(current_snapshot.sectoral_metrics)\n            recommendations.extend(sector_recs)\n            \n            # Analyze inclusion bias\n            inclusion_recs = await self._analyze_inclusion_bias(current_snapshot.inclusion_metrics)\n            recommendations.extend(inclusion_recs)\n            \n            # Analyze language bias\n            language_recs = await self._analyze_language_bias(current_snapshot.language_metrics)\n            recommendations.extend(language_recs)\n            \n            # Analyze stage bias\n            stage_recs = await self._analyze_stage_bias(current_snapshot.stage_metrics)\n            recommendations.extend(stage_recs)\n            \n            # Sort by priority\n            recommendations.sort(key=lambda x: x.get('priority', 0), reverse=True)\n            \n            return recommendations\n            \n        except Exception as e:\n            self.logger.error(f\"Getting bias recommendations failed: {e}\")\n            return []\n    \n    async def trigger_bias_mitigation(self, bias_type: BiasType, severity: AlertSeverity) -> Dict[str, Any]:\n        \"\"\"Trigger automatic bias mitigation measures\"\"\"\n        try:\n            mitigation_actions = []\n            \n            if bias_type == BiasType.GEOGRAPHIC:\n                if severity in [AlertSeverity.CRITICAL, AlertSeverity.HIGH]:\n                    # Boost underserved region searches\n                    mitigation_actions.append({\n                        'action': 'boost_underserved_searches',\n                        'description': 'Increase search frequency for Central/West Africa',\n                        'parameters': {'multiplier': 2.0, 'duration_hours': 24}\n                    })\n                    \n                    # Reduce Big 4 country search frequency\n                    mitigation_actions.append({\n                        'action': 'reduce_saturated_searches',\n                        'description': 'Reduce search frequency for oversaturated regions',\n                        'parameters': {'multiplier': 0.7, 'duration_hours': 24}\n                    })\n            \n            elif bias_type == BiasType.SECTORAL:\n                if severity in [AlertSeverity.CRITICAL, AlertSeverity.HIGH]:\n                    # Boost priority sector searches\n                    mitigation_actions.append({\n                        'action': 'boost_priority_sectors',\n                        'description': 'Increase searches for healthcare/agriculture/climate',\n                        'parameters': {'sectors': ['healthcare', 'agriculture', 'climate'], 'multiplier': 1.5}\n                    })\n            \n            elif bias_type == BiasType.LANGUAGE:\n                if severity in [AlertSeverity.CRITICAL, AlertSeverity.HIGH]:\n                    # Activate multilingual searches\n                    mitigation_actions.append({\n                        'action': 'activate_multilingual_search',\n                        'description': 'Activate French/Arabic/Portuguese searches',\n                        'parameters': {'languages': ['fr', 'ar', 'pt'], 'frequency': 'hourly'}\n                    })\n            \n            # Execute mitigation actions\n            execution_results = []\n            for action in mitigation_actions:\n                result = await self._execute_mitigation_action(action)\n                execution_results.append(result)\n            \n            return {\n                'bias_type': bias_type.value,\n                'severity': severity.value,\n                'mitigation_actions': mitigation_actions,\n                'execution_results': execution_results,\n                'timestamp': datetime.now().isoformat()\n            }\n            \n        except Exception as e:\n            self.logger.error(f\"Bias mitigation failed: {e}\")\n            return {'error': str(e)}\n    \n    # =============================================================================\n    # PRIVATE HELPER METHODS\n    # =============================================================================\n    \n    async def _get_current_ingestion_data(self) -> Dict[str, Any]:\n        \"\"\"Get current ingestion data for analysis\"\"\"\n        try:\n            async with get_db_session() as session:\n                # Get opportunities from last 24 hours\n                query = \"\"\"\n                    SELECT fo.*, \n                           vr.confidence_score,\n                           vr.validation_notes,\n                           cf.organization_name,\n                           cf.funding_amount,\n                           cf.key_phrases\n                    FROM funding_opportunities fo\n                    LEFT JOIN validation_results vr ON fo.id = vr.opportunity_id\n                    LEFT JOIN content_fingerprints cf ON fo.id = cf.opportunity_id\n                    WHERE fo.discovered_date >= (NOW() - INTERVAL '24 hours')\n                    ORDER BY fo.discovered_date DESC\n                \"\"\"\n                \n                result = await session.execute(query)\n                opportunities = [dict(row) for row in result.fetchall()]\n                \n                # Get source statistics\n                source_query = \"\"\"\n                    SELECT source_name, source_type, COUNT(*) as count\n                    FROM funding_opportunities\n                    WHERE discovered_date >= (NOW() - INTERVAL '24 hours')\n                    GROUP BY source_name, source_type\n                    ORDER BY count DESC\n                \"\"\"\n                \n                source_result = await session.execute(source_query)\n                source_stats = [dict(row) for row in source_result.fetchall()]\n                \n                return {\n                    'opportunities': opportunities,\n                    'source_stats': source_stats,\n                    'total_count': len(opportunities),\n                    'analysis_period': '24_hours'\n                }\n                \n        except Exception as e:\n            self.logger.error(f\"Getting current ingestion data failed: {e}\")\n            return {'opportunities': [], 'source_stats': [], 'total_count': 0}\n    \n    async def _calculate_geographic_metrics(self, data: Dict[str, Any]) -> Dict[str, BiasMetric]:\n        \"\"\"Calculate geographic bias metrics\"\"\"\n        try:\n            opportunities = data.get('opportunities', [])\n            if not opportunities:\n                return {}\n            \n            # Extract geographic data\n            geographic_data = defaultdict(int)\n            big_four_countries = {'KE', 'NG', 'ZA', 'EG'}\n            central_africa = {'CF', 'TD', 'CD', 'CM', 'GQ', 'GA'}\n            west_africa = {'GW', 'SL', 'LR', 'TG', 'BJ', 'NE', 'ML', 'BF', 'SN', 'CI', 'GH', 'GM', 'NG'}\n            \n            big_four_count = 0\n            central_africa_count = 0\n            west_africa_count = 0\n            total_with_location = 0\n            unique_countries = set()\n            \n            for opp in opportunities:\n                # Extract countries from opportunity data\n                countries = self._extract_countries_from_opportunity(opp)\n                \n                if countries:\n                    total_with_location += 1\n                    unique_countries.update(countries)\n                    \n                    for country in countries:\n                        geographic_data[country] += 1\n                        \n                        if country in big_four_countries:\n                            big_four_count += 1\n                        if country in central_africa:\n                            central_africa_count += 1\n                        if country in west_africa:\n                            west_africa_count += 1\n            \n            # Calculate metrics\n            metrics = {}\n            \n            if total_with_location > 0:\n                # Non-Big 4 percentage\n                non_big_four_pct = (total_with_location - big_four_count) / total_with_location\n                metrics['non_big_four_percentage'] = BiasMetric(\n                    metric_name='non_big_four_percentage',\n                    current_value=non_big_four_pct,\n                    target_value=self.bias_targets['geographic']['non_big_four_percentage']['target'],\n                    threshold_warning=self.bias_targets['geographic']['non_big_four_percentage']['warning'],\n                    threshold_critical=self.bias_targets['geographic']['non_big_four_percentage']['critical'],\n                    direction=BiasDirection.UNDER_REPRESENTED if non_big_four_pct < 0.35 else BiasDirection.BALANCED\n                )\n                \n                # Central Africa percentage\n                central_africa_pct = central_africa_count / total_with_location\n                metrics['central_africa_percentage'] = BiasMetric(\n                    metric_name='central_africa_percentage',\n                    current_value=central_africa_pct,\n                    target_value=self.bias_targets['geographic']['central_africa_percentage']['target'],\n                    threshold_warning=self.bias_targets['geographic']['central_africa_percentage']['warning'],\n                    threshold_critical=self.bias_targets['geographic']['central_africa_percentage']['critical'],\n                    direction=BiasDirection.UNDER_REPRESENTED if central_africa_pct < 0.15 else BiasDirection.BALANCED\n                )\n                \n                # West Africa percentage\n                west_africa_pct = west_africa_count / total_with_location\n                metrics['west_africa_percentage'] = BiasMetric(\n                    metric_name='west_africa_percentage',\n                    current_value=west_africa_pct,\n                    target_value=self.bias_targets['geographic']['west_africa_percentage']['target'],\n                    threshold_warning=self.bias_targets['geographic']['west_africa_percentage']['warning'],\n                    threshold_critical=self.bias_targets['geographic']['west_africa_percentage']['critical'],\n                    direction=BiasDirection.UNDER_REPRESENTED if west_africa_pct < 0.25 else BiasDirection.BALANCED\n                )\n                \n                # Geographic diversity index (Simpson's diversity index)\n                diversity_index = self._calculate_diversity_index(list(geographic_data.values()))\n                metrics['geographic_diversity_index'] = BiasMetric(\n                    metric_name='geographic_diversity_index',\n                    current_value=diversity_index,\n                    target_value=self.bias_targets['geographic']['geographic_diversity_index']['target'],\n                    threshold_warning=self.bias_targets['geographic']['geographic_diversity_index']['warning'],\n                    threshold_critical=self.bias_targets['geographic']['geographic_diversity_index']['critical'],\n                    direction=BiasDirection.UNDER_REPRESENTED if diversity_index < 0.7 else BiasDirection.BALANCED\n                )\n            \n            return metrics\n            \n        except Exception as e:\n            self.logger.error(f\"Calculating geographic metrics failed: {e}\")\n            return {}\n    \n    async def _calculate_sectoral_metrics(self, data: Dict[str, Any]) -> Dict[str, BiasMetric]:\n        \"\"\"Calculate sectoral bias metrics\"\"\"\n        try:\n            opportunities = data.get('opportunities', [])\n            if not opportunities:\n                return {}\n            \n            # Extract sectoral data\n            sectoral_data = defaultdict(int)\n            healthcare_count = 0\n            agriculture_count = 0\n            climate_count = 0\n            total_count = len(opportunities)\n            \n            for opp in opportunities:\n                sectors = self._extract_sectors_from_opportunity(opp)\n                \n                for sector in sectors:\n                    sectoral_data[sector] += 1\n                    \n                    if sector == 'healthcare':\n                        healthcare_count += 1\n                    elif sector == 'agriculture':\n                        agriculture_count += 1\n                    elif sector == 'climate':\n                        climate_count += 1\n            \n            # Calculate metrics\n            metrics = {}\n            \n            if total_count > 0:\n                # Healthcare percentage\n                healthcare_pct = healthcare_count / total_count\n                metrics['healthcare_percentage'] = BiasMetric(\n                    metric_name='healthcare_percentage',\n                    current_value=healthcare_pct,\n                    target_value=self.bias_targets['sectoral']['healthcare_percentage']['target'],\n                    threshold_warning=self.bias_targets['sectoral']['healthcare_percentage']['warning'],\n                    threshold_critical=self.bias_targets['sectoral']['healthcare_percentage']['critical'],\n                    direction=BiasDirection.UNDER_REPRESENTED if healthcare_pct < 0.15 else BiasDirection.BALANCED\n                )\n                \n                # Agriculture percentage\n                agriculture_pct = agriculture_count / total_count\n                metrics['agriculture_percentage'] = BiasMetric(\n                    metric_name='agriculture_percentage',\n                    current_value=agriculture_pct,\n                    target_value=self.bias_targets['sectoral']['agriculture_percentage']['target'],\n                    threshold_warning=self.bias_targets['sectoral']['agriculture_percentage']['warning'],\n                    threshold_critical=self.bias_targets['sectoral']['agriculture_percentage']['critical'],\n                    direction=BiasDirection.UNDER_REPRESENTED if agriculture_pct < 0.15 else BiasDirection.BALANCED\n                )\n                \n                # Climate percentage\n                climate_pct = climate_count / total_count\n                metrics['climate_percentage'] = BiasMetric(\n                    metric_name='climate_percentage',\n                    current_value=climate_pct,\n                    target_value=self.bias_targets['sectoral']['climate_percentage']['target'],\n                    threshold_warning=self.bias_targets['sectoral']['climate_percentage']['warning'],\n                    threshold_critical=self.bias_targets['sectoral']['climate_percentage']['critical'],\n                    direction=BiasDirection.UNDER_REPRESENTED if climate_pct < 0.10 else BiasDirection.BALANCED\n                )\n                \n                # Sector diversity index\n                diversity_index = self._calculate_diversity_index(list(sectoral_data.values()))\n                metrics['sector_diversity_index'] = BiasMetric(\n                    metric_name='sector_diversity_index',\n                    current_value=diversity_index,\n                    target_value=self.bias_targets['sectoral']['sector_diversity_index']['target'],\n                    threshold_warning=self.bias_targets['sectoral']['sector_diversity_index']['warning'],\n                    threshold_critical=self.bias_targets['sectoral']['sector_diversity_index']['critical'],\n                    direction=BiasDirection.UNDER_REPRESENTED if diversity_index < 0.8 else BiasDirection.BALANCED\n                )\n            \n            return metrics\n            \n        except Exception as e:\n            self.logger.error(f\"Calculating sectoral metrics failed: {e}\")\n            return {}\n    \n    async def _calculate_inclusion_metrics(self, data: Dict[str, Any]) -> Dict[str, BiasMetric]:\n        \"\"\"Calculate inclusion bias metrics\"\"\"\n        try:\n            opportunities = data.get('opportunities', [])\n            if not opportunities:\n                return {}\n            \n            # Extract inclusion data\n            women_focused_count = 0\n            youth_focused_count = 0\n            rural_focused_count = 0\n            total_count = len(opportunities)\n            \n            for opp in opportunities:\n                inclusion_indicators = self._extract_inclusion_from_opportunity(opp)\n                \n                if 'women_led' in inclusion_indicators:\n                    women_focused_count += 1\n                if 'youth_focused' in inclusion_indicators:\n                    youth_focused_count += 1\n                if 'rural_priority' in inclusion_indicators:\n                    rural_focused_count += 1\n            \n            # Calculate metrics\n            metrics = {}\n            \n            if total_count > 0:\n                # Women-focused percentage\n                women_pct = women_focused_count / total_count\n                metrics['women_focused_percentage'] = BiasMetric(\n                    metric_name='women_focused_percentage',\n                    current_value=women_pct,\n                    target_value=self.bias_targets['inclusion']['women_focused_percentage']['target'],\n                    threshold_warning=self.bias_targets['inclusion']['women_focused_percentage']['warning'],\n                    threshold_critical=self.bias_targets['inclusion']['women_focused_percentage']['critical'],\n                    direction=BiasDirection.UNDER_REPRESENTED if women_pct < 0.20 else BiasDirection.BALANCED\n                )\n                \n                # Youth-focused percentage\n                youth_pct = youth_focused_count / total_count\n                metrics['youth_focused_percentage'] = BiasMetric(\n                    metric_name='youth_focused_percentage',\n                    current_value=youth_pct,\n                    target_value=self.bias_targets['inclusion']['youth_focused_percentage']['target'],\n                    threshold_warning=self.bias_targets['inclusion']['youth_focused_percentage']['warning'],\n                    threshold_critical=self.bias_targets['inclusion']['youth_focused_percentage']['critical'],\n                    direction=BiasDirection.UNDER_REPRESENTED if youth_pct < 0.25 else BiasDirection.BALANCED\n                )\n                \n                # Rural-focused percentage\n                rural_pct = rural_focused_count / total_count\n                metrics['rural_focused_percentage'] = BiasMetric(\n                    metric_name='rural_focused_percentage',\n                    current_value=rural_pct,\n                    target_value=self.bias_targets['inclusion']['rural_focused_percentage']['target'],\n                    threshold_warning=self.bias_targets['inclusion']['rural_focused_percentage']['warning'],\n                    threshold_critical=self.bias_targets['inclusion']['rural_focused_percentage']['critical'],\n                    direction=BiasDirection.UNDER_REPRESENTED if rural_pct < 0.15 else BiasDirection.BALANCED\n                )\n            \n            return metrics\n            \n        except Exception as e:\n            self.logger.error(f\"Calculating inclusion metrics failed: {e}\")\n            return {}\n    \n    async def _calculate_language_metrics(self, data: Dict[str, Any]) -> Dict[str, BiasMetric]:\n        \"\"\"Calculate language bias metrics\"\"\"\n        try:\n            opportunities = data.get('opportunities', [])\n            if not opportunities:\n                return {}\n            \n            # Extract language data\n            language_data = defaultdict(int)\n            non_english_count = 0\n            french_count = 0\n            arabic_count = 0\n            total_count = len(opportunities)\n            \n            for opp in opportunities:\n                # Simple language detection based on content\n                language = self._detect_opportunity_language(opp)\n                language_data[language] += 1\n                \n                if language != 'en':\n                    non_english_count += 1\n                if language == 'fr':\n                    french_count += 1\n                if language == 'ar':\n                    arabic_count += 1\n            \n            # Calculate metrics\n            metrics = {}\n            \n            if total_count > 0:\n                # Non-English percentage\n                non_english_pct = non_english_count / total_count\n                metrics['non_english_percentage'] = BiasMetric(\n                    metric_name='non_english_percentage',\n                    current_value=non_english_pct,\n                    target_value=self.bias_targets['language']['non_english_percentage']['target'],\n                    threshold_warning=self.bias_targets['language']['non_english_percentage']['warning'],\n                    threshold_critical=self.bias_targets['language']['non_english_percentage']['critical'],\n                    direction=BiasDirection.UNDER_REPRESENTED if non_english_pct < 0.20 else BiasDirection.BALANCED\n                )\n                \n                # French percentage\n                french_pct = french_count / total_count\n                metrics['french_percentage'] = BiasMetric(\n                    metric_name='french_percentage',\n                    current_value=french_pct,\n                    target_value=self.bias_targets['language']['french_percentage']['target'],\n                    threshold_warning=self.bias_targets['language']['french_percentage']['warning'],\n                    threshold_critical=self.bias_targets['language']['french_percentage']['critical'],\n                    direction=BiasDirection.UNDER_REPRESENTED if french_pct < 0.10 else BiasDirection.BALANCED\n                )\n                \n                # Arabic percentage\n                arabic_pct = arabic_count / total_count\n                metrics['arabic_percentage'] = BiasMetric(\n                    metric_name='arabic_percentage',\n                    current_value=arabic_pct,\n                    target_value=self.bias_targets['language']['arabic_percentage']['target'],\n                    threshold_warning=self.bias_targets['language']['arabic_percentage']['warning'],\n                    threshold_critical=self.bias_targets['language']['arabic_percentage']['critical'],\n                    direction=BiasDirection.UNDER_REPRESENTED if arabic_pct < 0.05 else BiasDirection.BALANCED\n                )\n            \n            return metrics\n            \n        except Exception as e:\n            self.logger.error(f\"Calculating language metrics failed: {e}\")\n            return {}\n    \n    async def _calculate_stage_metrics(self, data: Dict[str, Any]) -> Dict[str, BiasMetric]:\n        \"\"\"Calculate funding stage bias metrics\"\"\"\n        try:\n            opportunities = data.get('opportunities', [])\n            if not opportunities:\n                return {}\n            \n            # Extract stage data\n            stage_data = defaultdict(int)\n            early_stage_count = 0\n            seed_stage_count = 0\n            grant_count = 0\n            total_count = len(opportunities)\n            \n            for opp in opportunities:\n                stage = self._extract_stage_from_opportunity(opp)\n                stage_data[stage] += 1\n                \n                if stage in ['pre_seed', 'seed']:\n                    early_stage_count += 1\n                if stage == 'seed':\n                    seed_stage_count += 1\n                if stage == 'grant':\n                    grant_count += 1\n            \n            # Calculate metrics\n            metrics = {}\n            \n            if total_count > 0:\n                # Early stage percentage\n                early_stage_pct = early_stage_count / total_count\n                metrics['early_stage_percentage'] = BiasMetric(\n                    metric_name='early_stage_percentage',\n                    current_value=early_stage_pct,\n                    target_value=self.bias_targets['funding_stage']['early_stage_percentage']['target'],\n                    threshold_warning=self.bias_targets['funding_stage']['early_stage_percentage']['warning'],\n                    threshold_critical=self.bias_targets['funding_stage']['early_stage_percentage']['critical'],\n                    direction=BiasDirection.UNDER_REPRESENTED if early_stage_pct < 0.40 else BiasDirection.BALANCED\n                )\n                \n                # Seed stage percentage\n                seed_stage_pct = seed_stage_count / total_count\n                metrics['seed_stage_percentage'] = BiasMetric(\n                    metric_name='seed_stage_percentage',\n                    current_value=seed_stage_pct,\n                    target_value=self.bias_targets['funding_stage']['seed_stage_percentage']['target'],\n                    threshold_warning=self.bias_targets['funding_stage']['seed_stage_percentage']['warning'],\n                    threshold_critical=self.bias_targets['funding_stage']['seed_stage_percentage']['critical'],\n                    direction=BiasDirection.UNDER_REPRESENTED if seed_stage_pct < 0.25 else BiasDirection.BALANCED\n                )\n                \n                # Grant percentage\n                grant_pct = grant_count / total_count\n                metrics['grant_percentage'] = BiasMetric(\n                    metric_name='grant_percentage',\n                    current_value=grant_pct,\n                    target_value=self.bias_targets['funding_stage']['grant_percentage']['target'],\n                    threshold_warning=self.bias_targets['funding_stage']['grant_percentage']['warning'],\n                    threshold_critical=self.bias_targets['funding_stage']['grant_percentage']['critical'],\n                    direction=BiasDirection.UNDER_REPRESENTED if grant_pct < 0.30 else BiasDirection.BALANCED\n                )\n            \n            return metrics\n            \n        except Exception as e:\n            self.logger.error(f\"Calculating stage metrics failed: {e}\")\n            return {}\n    \n    async def _calculate_source_metrics(self, data: Dict[str, Any]) -> Dict[str, BiasMetric]:\n        \"\"\"Calculate source quality bias metrics\"\"\"\n        try:\n            # This would integrate with the source quality scoring system\n            # For now, return empty metrics\n            return {}\n            \n        except Exception as e:\n            self.logger.error(f\"Calculating source metrics failed: {e}\")\n            return {}\n    \n    def _extract_countries_from_opportunity(self, opportunity: Dict[str, Any]) -> List[str]:\n        \"\"\"Extract countries from opportunity data\"\"\"\n        countries = []\n        \n        # Extract from title and description\n        title = opportunity.get('title', '')\n        description = opportunity.get('description', '')\n        content = f\"{title} {description}\".lower()\n        \n        # Simple country detection\n        country_patterns = {\n            'KE': ['kenya', 'nairobi'],\n            'NG': ['nigeria', 'lagos', 'abuja'],\n            'ZA': ['south africa', 'cape town', 'johannesburg'],\n            'EG': ['egypt', 'cairo'],\n            'GH': ['ghana', 'accra'],\n            'ET': ['ethiopia', 'addis ababa'],\n            'MA': ['morocco', 'casablanca'],\n            'TN': ['tunisia', 'tunis'],\n            'SN': ['senegal', 'dakar'],\n            'CI': ['ivory coast', 'côte d\\'ivoire', 'abidjan'],\n            'CF': ['central african republic', 'bangui'],\n            'TD': ['chad', 'n\\'djamena'],\n            'CD': ['democratic republic of congo', 'kinshasa'],\n            'CM': ['cameroon', 'yaoundé', 'douala']\n        }\n        \n        for iso_code, patterns in country_patterns.items():\n            for pattern in patterns:\n                if pattern in content:\n                    countries.append(iso_code)\n                    break\n        \n        return countries\n    \n    def _extract_sectors_from_opportunity(self, opportunity: Dict[str, Any]) -> List[str]:\n        \"\"\"Extract sectors from opportunity data\"\"\"\n        sectors = []\n        \n        # Extract from title and description\n        title = opportunity.get('title', '')\n        description = opportunity.get('description', '')\n        content = f\"{title} {description}\".lower()\n        \n        # Sector detection patterns\n        sector_patterns = {\n            'healthcare': ['health', 'medical', 'hospital', 'clinic', 'disease'],\n            'agriculture': ['agriculture', 'farming', 'crop', 'livestock', 'food'],\n            'climate': ['climate', 'environment', 'sustainable', 'green', 'renewable'],\n            'education': ['education', 'learning', 'school', 'university', 'training'],\n            'fintech': ['fintech', 'financial', 'banking', 'payment', 'credit']\n        }\n        \n        for sector, patterns in sector_patterns.items():\n            for pattern in patterns:\n                if pattern in content:\n                    sectors.append(sector)\n                    break\n        \n        return sectors\n    \n    def _extract_inclusion_from_opportunity(self, opportunity: Dict[str, Any]) -> List[str]:\n        \"\"\"Extract inclusion indicators from opportunity data\"\"\"\n        indicators = []\n        \n        # Extract from title and description\n        title = opportunity.get('title', '')\n        description = opportunity.get('description', '')\n        content = f\"{title} {description}\".lower()\n        \n        # Inclusion detection patterns\n        inclusion_patterns = {\n            'women_led': ['women', 'female', 'gender', 'maternal'],\n            'youth_focused': ['youth', 'young', 'student', 'under 35'],\n            'rural_priority': ['rural', 'remote', 'village', 'countryside'],\n            'disability_inclusive': ['disability', 'accessible', 'inclusive'],\n            'refugee_focused': ['refugee', 'displaced', 'migration']\n        }\n        \n        for indicator, patterns in inclusion_patterns.items():\n            for pattern in patterns:\n                if pattern in content:\n                    indicators.append(indicator)\n                    break\n        \n        return indicators\n    \n    def _detect_opportunity_language(self, opportunity: Dict[str, Any]) -> str:\n        \"\"\"Detect language of opportunity content\"\"\"\n        # Extract from title and description\n        title = opportunity.get('title', '')\n        description = opportunity.get('description', '')\n        content = f\"{title} {description}\"\n        \n        # Simple language detection patterns\n        language_patterns = {\n            'fr': ['financement', 'subvention', 'recherche', 'développement', 'programme'],\n            'ar': ['تمويل', 'منح', 'برامج', 'تطوير', 'ابتكار'],\n            'pt': ['financiamento', 'bolsa', 'investigação', 'desenvolvimento', 'programa'],\n            'sw': ['ufumuzi', 'ruzuku', 'utafiti', 'maendeleo', 'programu']\n        }\n        \n        for lang, patterns in language_patterns.items():\n            for pattern in patterns:\n                if pattern in content:\n                    return lang\n        \n        return 'en'  # Default to English\n    \n    def _extract_stage_from_opportunity(self, opportunity: Dict[str, Any]) -> str:\n        \"\"\"Extract funding stage from opportunity data\"\"\"\n        # Extract from title and description\n        title = opportunity.get('title', '')\n        description = opportunity.get('description', '')\n        content = f\"{title} {description}\".lower()\n        \n        # Stage detection patterns\n        stage_patterns = {\n            'pre_seed': ['pre-seed', 'idea', 'concept', 'prototype', 'mvp'],\n            'seed': ['seed', 'early stage', 'startup', 'launch'],\n            'series_a': ['series a', 'growth', 'scaling', 'expansion'],\n            'grant': ['grant', 'research', 'development', 'non-dilutive']\n        }\n        \n        for stage, patterns in stage_patterns.items():\n            for pattern in patterns:\n                if pattern in content:\n                    return stage\n        \n        return 'unknown'\n    \n    def _calculate_diversity_index(self, values: List[int]) -> float:\n        \"\"\"Calculate Simpson's diversity index\"\"\"\n        if not values or sum(values) == 0:\n            return 0.0\n        \n        total = sum(values)\n        sum_squares = sum(x * x for x in values)\n        \n        # Simpson's diversity index: 1 - (sum of squares / total squared)\n        diversity = 1 - (sum_squares / (total * total))\n        return diversity\n    \n    async def _calculate_overall_equity_score(self, all_metrics: Dict[str, Dict[str, BiasMetric]]) -> float:\n        \"\"\"Calculate overall equity score\"\"\"\n        try:\n            all_scores = []\n            \n            for metric_group in all_metrics.values():\n                for metric in metric_group.values():\n                    # Score based on how close to target\n                    deviation = abs(metric.current_value - metric.target_value)\n                    max_deviation = max(metric.threshold_warning, metric.threshold_critical)\n                    \n                    if max_deviation > 0:\n                        score = max(0.0, 1.0 - (deviation / max_deviation))\n                        all_scores.append(score)\n            \n            return mean(all_scores) if all_scores else 0.0\n            \n        except Exception as e:\n            self.logger.error(f\"Calculating overall equity score failed: {e}\")\n            return 0.0\n    \n    async def _generate_alerts(self, all_metrics: Dict[str, Dict[str, BiasMetric]]) -> List[BiasAlert]:\n        \"\"\"Generate bias alerts\"\"\"\n        alerts = []\n        \n        try:\n            for bias_type_str, metrics in all_metrics.items():\n                bias_type = BiasType(bias_type_str)\n                \n                for metric_name, metric in metrics.items():\n                    severity = metric.calculate_severity()\n                    \n                    if severity in [AlertSeverity.CRITICAL, AlertSeverity.HIGH]:\n                        alert = BiasAlert(\n                            alert_id=f\"{bias_type_str}_{metric_name}_{datetime.now().strftime('%Y%m%d_%H%M%S')}\",\n                            bias_type=bias_type,\n                            severity=severity,\n                            message=f\"{metric.metric_name}: {metric.current_value:.3f} (target: {metric.target_value:.3f})\",\n                            metric=metric,\n                            recommendations=await self._get_metric_recommendations(bias_type, metric_name, metric)\n                        )\n                        alerts.append(alert)\n            \n            return alerts\n            \n        except Exception as e:\n            self.logger.error(f\"Generating alerts failed: {e}\")\n            return []\n    \n    async def _get_metric_recommendations(self, bias_type: BiasType, metric_name: str, \n                                        metric: BiasMetric) -> List[str]:\n        \"\"\"Get recommendations for a specific metric\"\"\"\n        recommendations = []\n        \n        if bias_type == BiasType.GEOGRAPHIC:\n            if 'central_africa' in metric_name:\n                recommendations.extend([\n                    \"Increase search frequency for Central African sources\",\n                    \"Add Central African funding databases to monitoring\",\n                    \"Implement French-language searches for CAR/Chad/DRC\"\n                ])\n            elif 'non_big_four' in metric_name:\n                recommendations.extend([\n                    \"Reduce search frequency for Big 4 countries\",\n                    \"Boost searches for underrepresented countries\",\n                    \"Add regional development bank sources\"\n                ])\n        \n        elif bias_type == BiasType.SECTORAL:\n            if 'healthcare' in metric_name:\n                recommendations.extend([\n                    \"Add healthcare-specific funding sources\",\n                    \"Implement health-focused search terms\",\n                    \"Monitor WHO and health ministry announcements\"\n                ])\n            elif 'agriculture' in metric_name:\n                recommendations.extend([\n                    \"Add agricultural development funding sources\",\n                    \"Monitor FAO and agricultural ministry announcements\",\n                    \"Implement agri-tech specific search terms\"\n                ])\n        \n        elif bias_type == BiasType.LANGUAGE:\n            if 'french' in metric_name:\n                recommendations.extend([\n                    \"Activate French-language search modules\",\n                    \"Add francophone African funding sources\",\n                    \"Monitor AFD and francophone development banks\"\n                ])\n            elif 'arabic' in metric_name:\n                recommendations.extend([\n                    \"Activate Arabic-language search modules\",\n                    \"Add North African funding sources\",\n                    \"Monitor Islamic Development Bank announcements\"\n                ])\n        \n        return recommendations\n    \n    async def _store_snapshot(self, snapshot: BiasSnapshot):\n        \"\"\"Store bias snapshot for historical analysis\"\"\"\n        try:\n            # Store in historical snapshots\n            self.historical_snapshots.append(snapshot)\n            \n            # Trim old snapshots\n            cutoff_date = datetime.now() - timedelta(days=self.max_history_days)\n            self.historical_snapshots = [\n                s for s in self.historical_snapshots \n                if s.timestamp > cutoff_date\n            ]\n            \n        except Exception as e:\n            self.logger.error(f\"Storing snapshot failed: {e}\")\n    \n    async def _get_historical_snapshots(self, days: int) -> List[BiasSnapshot]:\n        \"\"\"Get historical snapshots for trend analysis\"\"\"\n        try:\n            cutoff_date = datetime.now() - timedelta(days=days)\n            return [s for s in self.historical_snapshots if s.timestamp > cutoff_date]\n            \n        except Exception as e:\n            self.logger.error(f\"Getting historical snapshots failed: {e}\")\n            return []\n    \n    def _calculate_metric_trends(self, historical_data: List[BiasSnapshot], \n                               metric_group: str) -> Dict[str, Any]:\n        \"\"\"Calculate trends for a metric group\"\"\"\n        try:\n            if not historical_data:\n                return {}\n            \n            trends = {}\n            \n            # Get all metric names from the group\n            all_metrics = set()\n            for snapshot in historical_data:\n                group_metrics = getattr(snapshot, metric_group, {})\n                all_metrics.update(group_metrics.keys())\n            \n            # Calculate trend for each metric\n            for metric_name in all_metrics:\n                values = []\n                for snapshot in historical_data:\n                    group_metrics = getattr(snapshot, metric_group, {})\n                    metric = group_metrics.get(metric_name)\n                    if metric:\n                        values.append(metric.current_value)\n                \n                if len(values) >= 2:\n                    # Simple trend calculation\n                    first_half = values[:len(values)//2]\n                    second_half = values[len(values)//2:]\n                    \n                    first_avg = mean(first_half)\n                    second_avg = mean(second_half)\n                    \n                    trend_direction = 'improving' if second_avg > first_avg else 'declining'\n                    trend_magnitude = abs(second_avg - first_avg)\n                    \n                    trends[metric_name] = {\n                        'direction': trend_direction,\n                        'magnitude': trend_magnitude,\n                        'current_value': values[-1],\n                        'volatility': stdev(values) if len(values) > 1 else 0.0\n                    }\n            \n            return trends\n            \n        except Exception as e:\n            self.logger.error(f\"Calculating metric trends failed: {e}\")\n            return {}\n    \n    def _calculate_equity_trend(self, historical_data: List[BiasSnapshot]) -> Dict[str, Any]:\n        \"\"\"Calculate overall equity trend\"\"\"\n        try:\n            if not historical_data:\n                return {}\n            \n            equity_scores = [s.overall_equity_score for s in historical_data]\n            \n            if len(equity_scores) >= 2:\n                first_half = equity_scores[:len(equity_scores)//2]\n                second_half = equity_scores[len(equity_scores)//2:]\n                \n                first_avg = mean(first_half)\n                second_avg = mean(second_half)\n                \n                return {\n                    'direction': 'improving' if second_avg > first_avg else 'declining',\n                    'magnitude': abs(second_avg - first_avg),\n                    'current_score': equity_scores[-1],\n                    'volatility': stdev(equity_scores) if len(equity_scores) > 1 else 0.0\n                }\n            \n            return {'direction': 'unknown', 'magnitude': 0.0}\n            \n        except Exception as e:\n            self.logger.error(f\"Calculating equity trend failed: {e}\")\n            return {}\n    \n    async def _analyze_geographic_bias(self, metrics: Dict[str, BiasMetric]) -> List[Dict[str, Any]]:\n        \"\"\"Analyze geographic bias and provide recommendations\"\"\"\n        recommendations = []\n        \n        for metric_name, metric in metrics.items():\n            if metric.direction == BiasDirection.UNDER_REPRESENTED:\n                if 'central_africa' in metric_name:\n                    recommendations.append({\n                        'type': 'geographic_bias',\n                        'priority': 5,\n                        'title': 'Central Africa Under-represented',\n                        'description': f'Only {metric.current_value:.1%} of opportunities from Central Africa (target: {metric.target_value:.1%})',\n                        'actions': [\n                            'Add Central African funding databases',\n                            'Implement French-language searches',\n                            'Monitor regional development banks'\n                        ]\n                    })\n                elif 'non_big_four' in metric_name:\n                    recommendations.append({\n                        'type': 'geographic_bias',\n                        'priority': 4,\n                        'title': 'Over-concentration in Big 4 Countries',\n                        'description': f'Only {metric.current_value:.1%} of opportunities from non-Big 4 countries (target: {metric.target_value:.1%})',\n                        'actions': [\n                            'Reduce Big 4 search frequency',\n                            'Boost underserved region searches',\n                            'Add regional funding sources'\n                        ]\n                    })\n        \n        return recommendations\n    \n    async def _analyze_sectoral_bias(self, metrics: Dict[str, BiasMetric]) -> List[Dict[str, Any]]:\n        \"\"\"Analyze sectoral bias and provide recommendations\"\"\"\n        recommendations = []\n        \n        for metric_name, metric in metrics.items():\n            if metric.direction == BiasDirection.UNDER_REPRESENTED:\n                if 'healthcare' in metric_name:\n                    recommendations.append({\n                        'type': 'sectoral_bias',\n                        'priority': 5,\n                        'title': 'Healthcare Sector Under-represented',\n                        'description': f'Only {metric.current_value:.1%} of opportunities in healthcare (target: {metric.target_value:.1%})',\n                        'actions': [\n                            'Add health ministry sources',\n                            'Monitor WHO funding announcements',\n                            'Implement health-specific search terms'\n                        ]\n                    })\n                elif 'agriculture' in metric_name:\n                    recommendations.append({\n                        'type': 'sectoral_bias',\n                        'priority': 5,\n                        'title': 'Agriculture Sector Under-represented',\n                        'description': f'Only {metric.current_value:.1%} of opportunities in agriculture (target: {metric.target_value:.1%})',\n                        'actions': [\n                            'Add agricultural ministry sources',\n                            'Monitor FAO funding announcements',\n                            'Implement agri-tech search terms'\n                        ]\n                    })\n        \n        return recommendations\n    \n    async def _analyze_inclusion_bias(self, metrics: Dict[str, BiasMetric]) -> List[Dict[str, Any]]:\n        \"\"\"Analyze inclusion bias and provide recommendations\"\"\"\n        recommendations = []\n        \n        for metric_name, metric in metrics.items():\n            if metric.direction == BiasDirection.UNDER_REPRESENTED:\n                if 'women' in metric_name:\n                    recommendations.append({\n                        'type': 'inclusion_bias',\n                        'priority': 4,\n                        'title': 'Women-focused Opportunities Under-represented',\n                        'description': f'Only {metric.current_value:.1%} of opportunities target women (target: {metric.target_value:.1%})',\n                        'actions': [\n                            'Add women-focused funding sources',\n                            'Monitor gender equality organizations',\n                            'Implement women-specific search terms'\n                        ]\n                    })\n                elif 'youth' in metric_name:\n                    recommendations.append({\n                        'type': 'inclusion_bias',\n                        'priority': 4,\n                        'title': 'Youth-focused Opportunities Under-represented',\n                        'description': f'Only {metric.current_value:.1%} of opportunities target youth (target: {metric.target_value:.1%})',\n                        'actions': [\n                            'Add youth-focused funding sources',\n                            'Monitor youth development organizations',\n                            'Implement youth-specific search terms'\n                        ]\n                    })\n        \n        return recommendations\n    \n    async def _analyze_language_bias(self, metrics: Dict[str, BiasMetric]) -> List[Dict[str, Any]]:\n        \"\"\"Analyze language bias and provide recommendations\"\"\"\n        recommendations = []\n        \n        for metric_name, metric in metrics.items():\n            if metric.direction == BiasDirection.UNDER_REPRESENTED:\n                if 'french' in metric_name:\n                    recommendations.append({\n                        'type': 'language_bias',\n                        'priority': 3,\n                        'title': 'French Language Under-represented',\n                        'description': f'Only {metric.current_value:.1%} of opportunities in French (target: {metric.target_value:.1%})',\n                        'actions': [\n                            'Activate French search modules',\n                            'Add francophone funding sources',\n                            'Monitor AFD announcements'\n                        ]\n                    })\n                elif 'arabic' in metric_name:\n                    recommendations.append({\n                        'type': 'language_bias',\n                        'priority': 3,\n                        'title': 'Arabic Language Under-represented',\n                        'description': f'Only {metric.current_value:.1%} of opportunities in Arabic (target: {metric.target_value:.1%})',\n                        'actions': [\n                            'Activate Arabic search modules',\n                            'Add North African funding sources',\n                            'Monitor Islamic Development Bank'\n                        ]\n                    })\n        \n        return recommendations\n    \n    async def _analyze_stage_bias(self, metrics: Dict[str, BiasMetric]) -> List[Dict[str, Any]]:\n        \"\"\"Analyze funding stage bias and provide recommendations\"\"\"\n        recommendations = []\n        \n        for metric_name, metric in metrics.items():\n            if metric.direction == BiasDirection.UNDER_REPRESENTED:\n                if 'early_stage' in metric_name:\n                    recommendations.append({\n                        'type': 'stage_bias',\n                        'priority': 4,\n                        'title': 'Early Stage Opportunities Under-represented',\n                        'description': f'Only {metric.current_value:.1%} of opportunities are early-stage (target: {metric.target_value:.1%})',\n                        'actions': [\n                            'Add accelerator/incubator sources',\n                            'Monitor startup competition announcements',\n                            'Implement early-stage search terms'\n                        ]\n                    })\n                elif 'grant' in metric_name:\n                    recommendations.append({\n                        'type': 'stage_bias',\n                        'priority': 4,\n                        'title': 'Grant Opportunities Under-represented',\n                        'description': f'Only {metric.current_value:.1%} of opportunities are grants (target: {metric.target_value:.1%})',\n                        'actions': [\n                            'Add government funding sources',\n                            'Monitor research funding announcements',\n                            'Implement grant-specific search terms'\n                        ]\n                    })\n        \n        return recommendations\n    \n    async def _execute_mitigation_action(self, action: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"Execute a specific mitigation action\"\"\"\n        try:\n            action_type = action.get('action')\n            \n            if action_type == 'boost_underserved_searches':\n                # This would integrate with the search scheduling system\n                return {\n                    'action': action_type,\n                    'status': 'executed',\n                    'message': 'Boosted underserved region searches',\n                    'timestamp': datetime.now().isoformat()\n                }\n            \n            elif action_type == 'activate_multilingual_search':\n                # This would integrate with the multilingual search system\n                return {\n                    'action': action_type,\n                    'status': 'executed',\n                    'message': 'Activated multilingual search modules',\n                    'timestamp': datetime.now().isoformat()\n                }\n            \n            else:\n                return {\n                    'action': action_type,\n                    'status': 'not_implemented',\n                    'message': 'Action type not implemented',\n                    'timestamp': datetime.now().isoformat()\n                }\n                \n        except Exception as e:\n            self.logger.error(f\"Executing mitigation action failed: {e}\")\n            return {\n                'action': action.get('action', 'unknown'),\n                'status': 'failed',\n                'message': str(e),\n                'timestamp': datetime.now().isoformat()\n            }\n\n# =============================================================================\n# USAGE EXAMPLE\n# =============================================================================\n\nasync def example_usage():\n    \"\"\"Example usage of bias monitoring system\"\"\"\n    monitor = BiasMonitoringEngine()\n    \n    # Analyze current bias\n    snapshot = await monitor.analyze_current_bias()\n    \n    print(\"=== Current Bias Analysis ===\")\n    print(f\"Overall Equity Score: {snapshot.overall_equity_score:.3f}\")\n    print(f\"Active Alerts: {len(snapshot.active_alerts)}\")\n    \n    # Show geographic metrics\n    print(\"\\n=== Geographic Metrics ===\")\n    for name, metric in snapshot.geographic_metrics.items():\n        print(f\"{name}: {metric.current_value:.3f} (target: {metric.target_value:.3f})\")\n    \n    # Show alerts\n    print(\"\\n=== Active Alerts ===\")\n    for alert in snapshot.active_alerts:\n        print(f\"[{alert.severity.value.upper()}] {alert.message}\")\n    \n    # Get recommendations\n    recommendations = await monitor.get_bias_recommendations()\n    print(f\"\\n=== Recommendations ({len(recommendations)}) ===\")\n    for rec in recommendations[:5]:  # Show top 5\n        print(f\"[Priority {rec['priority']}] {rec['title']}\")\n        print(f\"  {rec['description']}\")\n    \n    # Get trends\n    trends = await monitor.get_bias_trends(30)\n    print(f\"\\n=== 30-Day Trends ===\")\n    equity_trend = trends.get('overall_equity_trend', {})\n    print(f\"Overall Equity: {equity_trend.get('direction', 'unknown')} (magnitude: {equity_trend.get('magnitude', 0):.3f})\")\n\nif __name__ == \"__main__\":\n    asyncio.run(example_usage())"